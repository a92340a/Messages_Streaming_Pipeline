apiVersion: batch/v1
kind: Job
metadata:
  name: data-consumer
spec:
  template:
    metadata:
      labels:
        app: data-consumer
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: spark-pool
      securityContext:
        runAsUser: 0
      containers:
        - name: data-consumer
          image: a92340a/message_data_consumer:1.1
          command: ["/opt/bitnami/spark/bin/spark-submit"]
          args:
            - "--master"
            - "spark://spark-master-service:7077"
            - "--deploy-mode"
            - "client"
            - "--packages"
            - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.5"
            - "--conf"
            - "spark.jars.ivy=/tmp/.ivy2"
            - "--conf"
            - "spark.driver.bindAddress=0.0.0.0"
            - "--conf"
            - "spark.driver.host=data-consumer-service.default.svc.cluster.local"
            - "--conf"
            - "spark.executor.instances=1"
            - "--conf"
            - "spark.kubernetes.node.selector.spark-role=executor"
            - "--conf"
            - "spark.executor.cores=1"
            - "--conf"
            - "spark.executor.memory=8g"
            - "local:///app/data_consumer.py"
          env:
            - name: BOOTSTRAP_SERVERS
              value: "kafka-service:9092"
            - name: TOPIC
              value: "message_data"
            - name: HADOOP_CONF_DIR
              value: "/etc/hadoop/conf"
          envFrom:
            - configMapRef:
                name: postgres-config
          volumeMounts:
            - name: checkpoint-volume
              mountPath: /mnt/checkpoints/
      volumes:
        - name: checkpoint-volume
          persistentVolumeClaim:
            claimName: checkpoint-pvc
      restartPolicy: Never
  backoffLimit: 4
