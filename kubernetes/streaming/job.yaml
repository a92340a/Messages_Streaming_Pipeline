apiVersion: batch/v1
kind: Job
metadata:
  name: data-producer
spec:
  template:
    metadata:
      labels:
        app: data-producer
    spec:
      containers:
        - name: data-producer
          image: a92340a/message_data_producer:0.4
          env:
            - name: FILE_PATH
              value: "gs://messages-streaming-project-data/messages.csv"
            - name: BOOTSTRAP_SERVERS
              value: "kafka-service:9092"
      restartPolicy: Never
  backoffLimit: 4
---
apiVersion: batch/v1
kind: Job
metadata:
  name: data-consumer
spec:
  template:
    metadata:
      labels:
        app: data-consumer
    spec:
      securityContext:
        runAsUser: 0 #65534
        # runAsGroup: 65534
      containers:
        - name: data-consumer
          image: a92340a/message_data_consumer:0.9
          command: ["/opt/bitnami/spark/bin/spark-submit"]
          args:
            - "--master"
            - "spark://spark-master-service:7077"
            - "--deploy-mode"
            - "client"
            - "--packages"
            - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.5"
            - "--conf"
            - "spark.jars.ivy=/tmp/.ivy2"
            - "--conf"
            - "spark.driver.host=$(hostname -i)"
            - "local:///app/data_consumer.py"
          env:
            - name: BOOTSTRAP_SERVERS
              value: "kafka-service:9092"
            - name: TOPIC
              value: "message_data"
            - name: HADOOP_CONF_DIR
              value: "/etc/hadoop/conf"
          envFrom:
            - secretRef:
                name: postgres-secrets
      restartPolicy: Never
  backoffLimit: 4
